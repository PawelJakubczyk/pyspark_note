{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOmA/Mx0j9lOGEb3oy/jHfC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PawelJakubczyk/pyspark_note/blob/main/pyspark_crud_craete.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install Liblary"
      ],
      "metadata": {
        "id": "IjO7po58ZMLS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "op_JYUA9Yukr",
        "outputId": "c5a6d513-bfea-4b13-e92e-eeaf38f326e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.5.0)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creat DF"
      ],
      "metadata": {
        "id": "z2dUaWy1kzHy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import Library"
      ],
      "metadata": {
        "id": "qR-V3bvhZRJ6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZmHIGJsJYBCy"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DateType"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Start Spark Session"
      ],
      "metadata": {
        "id": "RPigNF2tZVk9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Python Spark SQL basic example\") \\\n",
        "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
        "    .getOrCreate()"
      ],
      "metadata": {
        "id": "YRckIPtCYxvm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define Schema"
      ],
      "metadata": {
        "id": "3ydjdUXQZfcN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "custom_schema = StructType(\n",
        "    [\n",
        "        StructField(\"ID\", StringType()),\n",
        "        StructField(\"Subsector\", StringType()),\n",
        "        StructField(\"Category\", StringType()),\n",
        "        StructField(\"Brand\", StringType()),\n",
        "        StructField(\"Material_ID\", IntegerType()),\n",
        "        StructField(\"Description\", StringType()),\n",
        "        StructField(\"Plant_Code\", StringType()),\n",
        "        StructField(\"Plant_Name\", StringType()),\n",
        "        StructField(\"Validity_Date_From\", DateType()),\n",
        "        StructField(\"Validity_Date_To\", DateType()),\n",
        "        StructField(\"Modification_Date\", DateType()),\n",
        "        StructField(\"Status\", StringType()),\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "1YSnZHxtZga1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create Empty DF"
      ],
      "metadata": {
        "id": "fR2euXSTyap7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "emptyRDD = spark.sparkContext.emptyRDD()\n",
        "df_empty = spark.createDataFrame(emptyRDD, custom_schema)\n",
        "\n",
        "df_empty = spark.createDataFrame([], custom_schema)"
      ],
      "metadata": {
        "id": "9tkyUMYUyaZM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create DF from variables"
      ],
      "metadata": {
        "id": "d28LZDdOy2pU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# list  of college data with two lists\n",
        "data = [[\"node.js\", \"dbms\", \"integration\"],\n",
        "        [\"jsp\", \"SQL\", \"trigonometry\"],\n",
        "        [\"php\", \"oracle\", \"statistics\"],\n",
        "        [\".net\", \"db2\", \"Machine Learning\"]]\n",
        "\n",
        "# giving column names of dataframe\n",
        "columns = [\"Web Technologies\", \"Data bases\", \"Maths\"]\n",
        "\n",
        "# creating a dataframe\n",
        "dataframe = spark.createDataFrame(data, columns, schema=custom_schema)"
      ],
      "metadata": {
        "id": "bEkYjFVjy-Yf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Craete DF from file"
      ],
      "metadata": {
        "id": "FaxAUlIFZcrt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### xlsx"
      ],
      "metadata": {
        "id": "jUOT7uovf-FS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "xlsx_file_path = \"path/to/your/xlsx/file.xlsx\"\n",
        "sheet_name = \"name_of_your_excel_sheet\"\n",
        "\n",
        "df_xlsx_pd = pd.read_excel(xlsx_file_path, sheet_name=sheet_name, inferSchema=True, schema=custom_schema)\n",
        "df_xlsx = spark.createDataFrame(df_xlsx_pd)\n",
        "\n",
        "df_xlsx = (spark.read.format(\"com.crealytics.spark.excel\")\n",
        "    .option(\"useHeader\", \"true\")\n",
        "    .option(\"inferSchema\", \"true\")\n",
        "    .option(\"dataAddress\", f\"'{sheet_name}'!\")\n",
        "    .schema(custom_schema)\n",
        "    .load(xlsx_file_path))"
      ],
      "metadata": {
        "id": "PAYZmF9LY31h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### csv"
      ],
      "metadata": {
        "id": "_a-dj38hf2iR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "csv_file_path = \"path/to/your/csv/file.csv\"\n",
        "df_csv = spark.read.csv(csv_file_path, schema=custom_schema, header=True, inferSchema=True)\n",
        "\n",
        "df_csv = (spark.read.format(\"csv\")\n",
        "    .option(\"inferSchema\", \"true\")\n",
        "    .option(\"header\", \"true\")\n",
        "    .schema(custom_schema)\n",
        "    .load(csv_file_path))"
      ],
      "metadata": {
        "id": "5lqVnve8f4qB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### json"
      ],
      "metadata": {
        "id": "3WQN-ENPhZdl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "json_file_path = \"path/to/your/json/file.json\"\n",
        "\n",
        "df_json = spark.read.json(json_file_path, schema=custom_schema, header=True, inferSchema=True)\n",
        "\n",
        "df_json = (spark.read.format(\"json\")\n",
        "    .option(\"inferSchema\", \"true\")\n",
        "    .schema(custom_schema)\n",
        "    .load(json_file_path))"
      ],
      "metadata": {
        "id": "uS69Pn5khY77"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### parquet"
      ],
      "metadata": {
        "id": "BA2d6Jcei7zB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "parquet_file_path = \"path/to/your/json/file.json\"\n",
        "\n",
        "df_hist_del = spark.read.parquet(parquet_file_path)\n",
        "df_read_history = spark.read.format(\"parquet\").load(parquet_file_path)"
      ],
      "metadata": {
        "id": "M6vg0OeDi-yR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### delta"
      ],
      "metadata": {
        "id": "pVJ7-9gOmIaX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "delta_files_path = \"path/to/your/delta/folder\"\n",
        "\n",
        "df_delta = spark.read.format(\"delta\").load(delta_files_path)\n",
        "df_delta = spark.read.delta.load(delta_files_path)"
      ],
      "metadata": {
        "id": "YT6WwbUHmKGa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### all format"
      ],
      "metadata": {
        "id": "KZ8RKtxAnSxo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path = \"path/to/your/file_or_folder\"\n",
        "\n",
        "def create_df(format:str, path:str, sheet_name:str = \"\", custom_schema = \"\") -> pyspark.sql.dataframe.DataFrame:\n",
        "    formats_list = [\"csv\", \"xlsx\", \"json\", \"parquet\", \"delta\"]\n",
        "    options = \"\"\n",
        "\n",
        "    if format not in formats_list:\n",
        "        raise ValueError(f\"the function only supports selected formats: {formats_list}\")\n",
        "\n",
        "    if format not in [\"parquet\", \"delta\"]:\n",
        "        if not custom_schema:\n",
        "            raise ValueError(\"the selected format does not store a schema, you must provide the schema argument\")\n",
        "        options += '.option(\"useHeader\", \"true\").option(\"inferSchema\", \"true\").schema(custom_schema)'\n",
        "\n",
        "        if format == \"xlsx\":\n",
        "            format == \"com.crealytics.spark.excel\"\n",
        "            if sheet_name == '':\n",
        "                raise ValueError(\"for the xlsx format, the sheets variable is required\")\n",
        "            ptions += f'.option(\"dataAddress\", f\"'{sheet_name}'!\")'\n",
        "\n",
        "    df = eval(f'spark.read.format({format}).{options}.load({path})')\n",
        "    return df"
      ],
      "metadata": {
        "id": "d92QhruMnZh9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eoJ-VIYQwq8u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bZ6rraNdwrXX",
        "outputId": "d2e3c2be-fe1a-4945-9310-d0b5a4cf8ff1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EmptyRDD[0] at emptyRDD at NativeMethodAccessorImpl.java:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vlHzFttYwuTW",
        "outputId": "aa2415e5-0e3a-45e0-ba35-d9c044975d43"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pyspark.rdd.RDD'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "schema = StructType([\n",
        "  StructField('firstname', StringType(), True),\n",
        "  StructField('middlename', StringType(), True),\n",
        "  StructField('lastname', StringType(), True)\n",
        "  ])\n",
        "\n",
        "df1 = emptyRDD.toDF(schema)\n",
        "print(type(df1))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PTYEn4i0wxQN",
        "outputId": "82cd0d4c-fd04-47c1-a7b9-4e7f2418806a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pyspark.sql.dataframe.DataFrame'>\n"
          ]
        }
      ]
    }
  ]
}